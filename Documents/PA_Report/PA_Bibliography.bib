@misc{Union2016,
author = {Union, Universal Postal},
pages = {2016},
title = {{History}},
url = {http://www.upu.int/en/the-upu/history/about-history.html},
urldate = {2017-11-18},
year = {2016}
}
@misc{Page2015,
author = {Page, Mark},
title = {{Why Do Humans Have Language? - The Atlantic}},
url = {https://www.theatlantic.com/business/archive/2015/06/why-humans-speak-language-origins/396635/},
urldate = {2017-11-18},
year = {2015}
}
@article{Sennrich2016,
abstract = {Neural machine translation (NMT) mod-els typically operate with a fixed vocabu-lary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this pa-per, we introduce a simpler and more ef-fective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as se-quences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via com-positional translation), and cognates and loanwords (via phonological and morpho-logical transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algo-rithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.07909v5},
author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
eprint = {arXiv:1508.07909v5},
file = {::},
title = {{Neural Machine Translation of Rare Words with Subword Units}},
url = {https://arxiv.org/pdf/1508.07909.pdf},
year = {2016}
}
@article{Sennrich2016a,
abstract = {Neural machine translation (NMT) mod-els typically operate with a fixed vocabu-lary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this pa-per, we introduce a simpler and more ef-fective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as se-quences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via com-positional translation), and cognates and loanwords (via phonological and morpho-logical transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algo-rithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.07909v5},
author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
eprint = {arXiv:1508.07909v5},
file = {::},
title = {{Neural Machine Translation of Rare Words with Subword Units}},
url = {https://arxiv.org/pdf/1508.07909.pdf},
year = {2016}
}
@article{Chen2014,
abstract = {BLEU is the de facto standard machine translation (MT) evaluation metric. How-ever, because BLEU computes a geo-metric mean of n-gram precisions, it of-ten correlates poorly with human judg-ment on the sentence-level. There-fore, several smoothing techniques have been proposed. This paper systemati-cally compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine transla-tion tuning.},
author = {Chen, Boxing and Cherry, Colin},
pages = {362--367},
title = {{A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU}},
url = {http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf},
year = {2014}
}
@article{Cho,
abstract = {In this paper, we propose a novel neu-ral network model called RNN Encoder– Decoder that consists of two recurrent neural networks (RNN). One RNN en-codes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another se-quence of symbols. The encoder and de-coder of the proposed model are jointly trained to maximize the conditional prob-ability of a target sequence given a source sequence. The performance of a statisti-cal machine translation system is empiri-cally found to improve by using the con-ditional probabilities of phrase pairs com-puted by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
file = {::},
title = {{Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation}},
url = {https://arxiv.org/pdf/1406.1078.pdf}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.3215v3},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {arXiv:1409.3215v3},
file = {::},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {https://arxiv.org/pdf/1409.3215.pdf},
year = {2014}
}
@article{Britz,
abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically re-quiring days to weeks of GPU time to converge. This makes exhaustive hyper-parameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analy-sis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architec-tures. As part of this contribution, we release an open-source NMT framework 1 that enables researchers to easily experi-ment with novel techniques and reproduce state of the art results.},
author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc and Brain, Google},
file = {::},
title = {{Massive Exploration of Neural Machine Translation Architectures}},
url = {https://arxiv.org/pdf/1703.03906.pdf}
}
@article{Cui,
abstract = {Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. Unlike web search, QA over a knowledge base gives out accurate and concise results, provid-ed that natural language questions can be understood and mapped precisely to structured queries over the knowledge base. The chal-lenge, however, is that a human can ask one question in many dif-ferent ways. Previous approaches have natural limits due to their representations: rule based approaches only understand a small set of " canned " questions, while keyword based or synonym based ap-proaches cannot fully understand the questions. In this paper, we design a new kind of question representation: templates, over a billion scale knowledge base and a million scale QA corpora. For example, for questions about a city's population, we learn tem-plates such as What's the population of {\$}city?, How many people are there in {\$}city?. We learned 27 mil-lion templates for 2782 intents. Based on these templates, our QA system KBQA effectively supports binary factoid questions, as well as complex questions which are composed of a series of binary fac-toid questions. Furthermore, we expand predicates in RDF knowl-edge base, which boosts the coverage of knowledge base by 57 times. Our QA system beats all other state-of-art works on both effectiveness and efficiency over QALD benchmarks.},
author = {Cui, Wanyun and Xiao, Yanghua and Wang, Haixun and Song, Yangqiu and Hwang, Seung-Won and Shanghai, Wei Wang},
file = {::},
title = {{KBQA: Learning Question Answering over QA Corpora and Knowledge Bases}},
url = {http://www.vldb.org/pvldb/vol10/p565-cui.pdf}
}
@article{Agarwal2017,
abstract = {We train a char2char model on the E2E NLG Challenge data, by exploiting " out-of-the-box " the recently released tf-seq2seq framework, using some of the standard options of this tool. With mini-mal effort, and in particular without delex-icalization, tokenization or lowercasing, the obtained raw predictions, according to a small scale human evaluation, are excel-lent on the linguistic side and quite rea-sonable on the adequacy side, the primary downside being the possible omissions of semantic material. However, in a signifi-cant number of cases (more than 70{\%}), a perfect solution can be found in the top-20 predictions, indicating promising direc-tions for solving the remaining issues.},
author = {Agarwal, Shubham and Dymetman, Marc},
file = {::},
pages = {158--163},
title = {{A surprisingly effective out-of-the-box char2char model on the E2E NLG Challenge dataset}},
url = {http://www.sigdial.org/workshops/conference18/proceedings/pdf/SIGDIAL19.pdf},
year = {2017}
}
@article{Novikova,
abstract = {This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation sys-tems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human refer-ence texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learn-ing from this dataset promises more nat-ural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.},
author = {Novikova, Jekaterina and Du{\v{s}}ek, Ondřej and Rieser, Verena},
file = {::},
title = {{The E2E Dataset: New Challenges For End-to-End Generation}},
url = {https://arxiv.org/pdf/1706.09254.pdf}
}
@article{Miller,
abstract = {Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, ques-tion answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the ad-dressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WIKIMOVIES, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WIKIQA benchmark.},
author = {Miller, Alexander H and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
file = {::},
title = {{Key-Value Memory Networks for Directly Reading Documents}},
url = {https://arxiv.org/pdf/1606.03126.pdf}
}
@article{Eric2017,
abstract = {Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.},
annote = {Zweck:

Paper auf dem unsere PA basiert.


Notizen Daniel:


Notizen Nicolas},
archivePrefix = {arXiv},
arxivId = {1705.05414},
author = {Eric, Mihail and Manning, Christopher D.},
eprint = {1705.05414},
file = {:C$\backslash$:/Users/daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eric, Manning - 2017 - Key-Value Retrieval Networks for Task-Oriented Dialogue.pdf:pdf},
month = {may},
title = {{Key-Value Retrieval Networks for Task-Oriented Dialogue}},
url = {http://arxiv.org/abs/1705.05414},
year = {2017}
}
