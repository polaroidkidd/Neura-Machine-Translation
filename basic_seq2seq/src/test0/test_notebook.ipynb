{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = '../data/fra.txt'\n",
    "MAX_SEQ_LEN = 250\n",
    "MAX_NUM_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "BASE_DATA_DIR = os.path.join(\"../../\", \"data\")\n",
    "BASIC_PERSISTENT_DIR = '/persistent/gpu2/'\n",
    "GRAPH_DIR = 'graph_stack2/'\n",
    "MODEL_DIR = 'model_stack2/'\n",
    "MODEL_CHECKPOINT_DIR = 'model_chkp_stack2/'\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "def load(file):\n",
    "    \"\"\"\n",
    "    Loads the given file into a list.\n",
    "    :param file: the file which should be loaded\n",
    "    :return: list of data\n",
    "    \"\"\"\n",
    "    with(open(file, encoding='utf8')) as file:\n",
    "        data = file.readlines()\n",
    "        # data = []\n",
    "        # for i in range(MAX_SENTENCES):\n",
    "        #    data.append(lines[i])\n",
    "    print('Loaded', len(data), \"lines of data.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(train_input_data, train_target_data, val_input_data, val_target_data):\n",
    "    train_input_data, train_target_data, val_input_data, val_target_data, word_index = tokenize(train_input_data,\n",
    "                                                                                                train_target_data,\n",
    "                                                                                                val_input_data,\n",
    "                                                                                                val_target_data)\n",
    "\n",
    "    train_input_data = pad_sequences(train_input_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "    train_target_data = pad_sequences(train_target_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "    val_input_data = pad_sequences(val_input_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "    val_target_data = pad_sequences(val_target_data, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "    embeddings_index = load_embedding()\n",
    "    embedding_matrix, num_words = prepare_embedding_matrix(word_index, embeddings_index)\n",
    "\n",
    "    # target_data = convert_last_dim_to_one_hot_enc(padded_target_data, num_words)\n",
    "\n",
    "    return train_input_data, train_target_data, val_input_data, val_target_data, embedding_matrix, num_words\n",
    "\n",
    "\n",
    "def tokenize(train_input_data, train_target_data, val_input_data, val_target_data):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(train_input_data + train_target_data + val_input_data + val_target_data)\n",
    "\n",
    "    train_input_data = tokenizer.texts_to_sequences(train_input_data)\n",
    "    train_target_data = tokenizer.texts_to_sequences(train_target_data)\n",
    "    val_input_data = tokenizer.texts_to_sequences(val_input_data)\n",
    "    val_target_data = tokenizer.texts_to_sequences(val_target_data)\n",
    "\n",
    "    return train_input_data, train_target_data, val_input_data, val_target_data, tokenizer.word_index\n",
    "\n",
    "\n",
    "def load_embedding():\n",
    "    print('Indexing word vectors.')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    filename = os.path.join(BASE_DATA_DIR, 'glove.6B.100d.txt')\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def prepare_embedding_matrix(word_index, embeddings_index):\n",
    "    print('Preparing embedding matrix.')\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix, num_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99999 lines of data.\n",
      "Loaded 99999 lines of data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "TRAIN_EN_FILE = \"train.en\"\n",
    "TRAIN_DE_FILE = \"train.de\"\n",
    "VAL_EN_FILE = \"newstest2014.en\"\n",
    "VAL_DE_FILE = \"newstest2014.de\"\n",
    "\n",
    "english_train_file = os.path.join(BASE_DATA_DIR, TRAIN_EN_FILE)\n",
    "german_train_file = os.path.join(BASE_DATA_DIR, TRAIN_DE_FILE)\n",
    "english_val_file = os.path.join(BASE_DATA_DIR, VAL_EN_FILE)\n",
    "german_val_file = os.path.join(BASE_DATA_DIR, VAL_DE_FILE)\n",
    "train_input_data = load(english_train_file)\n",
    "train_target_data = load(german_train_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2737 lines of data.\n",
      "Loaded 2737 lines of data.\n",
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "val_input_data = load(english_val_file)\n",
    "val_target_data = load(german_val_file)\n",
    "\n",
    "train_input_data, train_target_data, val_input_data, val_target_data, embedding_matrix, num_words = preprocess_data(\n",
    "    train_input_data, train_target_data, val_input_data, val_target_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoder_target_data(decoder_input_data):\n",
    "    print(\"decoder_input_data.shape\", decoder_input_data.shape)\n",
    "    decoder_target_data = np.zeros(decoder_input_data.shape)\n",
    "    print(\"decoder_target_data.shape\", decoder_target_data.shape)\n",
    "    for i in range(decoder_input_data.shape[0]):\n",
    "        for j in range(decoder_input_data.shape[1]-1):\n",
    "            decoder_target_data[i][j+1] = decoder_input_data[i][j]\n",
    "        decoder_target_data[i][0] = 0\n",
    "    return decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_data.shape (99999, 250)\n",
      "decoder_target_data.shape (99999, 250)\n"
     ]
    }
   ],
   "source": [
    "train_decoder_input_data = get_decoder_target_data(train_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99999, 250)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  2.,  3.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((2,3))\n",
    "a[0][0] = 1\n",
    "a[1][1] = 2\n",
    "a[1][2] = 3\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_data.shape (2, 3)\n",
      "decoder_target_data.shape (2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  2.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_decoder_target_data(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
